# üìä Proyecto Big Data - AgroDataSur

Sistema de an√°lisis de datos agr√≠colas que integra m√∫ltiples fuentes de datos (MySQL, MongoDB, API REST) y procesamiento en entorno Hadoop/Hive.

---

## üìÅ Estructura del Proyecto

```
BigData/
‚îú‚îÄ‚îÄ ArchivoConfiguracion/        # Archivos de configuraci√≥n de servicios
‚îÇ   ‚îú‚îÄ‚îÄ HDFS/                    # Configuraci√≥n Hadoop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core-site.xml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hdfs-site.xml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapred-site.xml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yarn-site.xml
‚îÇ   ‚îú‚îÄ‚îÄ Hive/                    # Configuraci√≥n Hive
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hive-site.xml
‚îÇ   ‚îî‚îÄ‚îÄ MONGODB/                 # Configuraci√≥n MongoDB
‚îÇ       ‚îú‚îÄ‚îÄ config.yml
‚îÇ       ‚îî‚îÄ‚îÄ mongod.service
‚îú‚îÄ‚îÄ generator.py                 # Generador de datos sint√©ticos
‚îú‚îÄ‚îÄ mydb.sql                     # Schema MySQL
‚îú‚îÄ‚îÄ poblado_tablas_completo.sql  # Datos MySQL (generado)
‚îú‚îÄ‚îÄ carga_mongodb.js             # Datos MongoDB (generado)
‚îú‚îÄ‚îÄ datos_sensores_api.sql       # Datos sensores IoT
‚îú‚îÄ‚îÄ etl.py                       # Pipeline ETL
‚îú‚îÄ‚îÄ consultas_hive.sql           # Consultas anal√≠ticas
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

---

## üìÅ Archivos de Datos y Scripts

### üîß **generator.py**
**Prop√≥sito:** Script generador de datos sint√©ticos para el proyecto.

**Funcionalidades:**
- Genera datos realistas para productos agr√≠colas con c√≥digos descriptivos (ej: `MAN-FJ-001`)
- Crea clientes con RUTs v√°lidos chilenos
- Genera transacciones de ventas con integridad referencial
- Produce datos de sensores IoT (temperatura, humedad, CO2)
- Genera opiniones de clientes con calificaciones y comentarios
- **Salidas generadas:**
  - `poblado_tablas_completo.sql`: Scripts INSERT para MySQL
  - `carga_mongodb.js`: Datos de opiniones para MongoDB

**Configuraci√≥n:**
- `CANTIDAD_VENTAS`: 220 registros (m√≠nimo 200)
- `CANTIDAD_CLIENTES`: 50 clientes
- `CANTIDAD_OPINIONES`: 220 opiniones
- 10 productos agr√≠colas base
- 6 sucursales en La Araucan√≠a

**Uso:**
```powershell
python generator.py
```

---

### üóÑÔ∏è **mydb.sql**
**Prop√≥sito:** Script de definici√≥n de esquema para base de datos relacional.

**Contenido:**
- Crea la base de datos `AgroDataSur`
- Define 4 tablas principales:
  - `clientes`: Informaci√≥n de clientes (RUT, nombre, regi√≥n)
  - `productos`: Cat√°logo de productos con c√≥digos y precios
  - `stock`: Inventario por sucursal con relaciones FK
  - `ventas`: Transacciones que relacionan clientes, productos y sucursales

**Relaciones:**
- Integridad referencial con claves for√°neas
- Normalizaci√≥n adecuada (3FN)

**Uso:**
```powershell
mysql -u root -p < mydb.sql
```

---

### üì• **poblado_tablas_completo.sql**
**Prop√≥sito:** Script de carga masiva de datos generados.

**Contenido:**
- Inserts para todas las tablas de MySQL
- Datos coherentes con integridad referencial
- Generado autom√°ticamente por `generator.py`

**Incluye:**
- 10 productos
- 50 clientes
- 60 registros de stock (10 productos √ó 6 sucursales)
- 220+ ventas

**Uso:**
```powershell
mysql -u root -p AgroDataSur < poblado_tablas_completo.sql
```

---

### üçÉ **carga_mongodb.js**
**Prop√≥sito:** Script de carga de datos NoSQL para MongoDB.

**Contenido:**
- Colecci√≥n: `opiniones_clientes` en DB `agrodata_opiniones`
- Documentos JSON con opiniones de clientes
- Campos: id_cliente, nombre, regi√≥n, producto, calificaci√≥n (1-5), comentario, fecha
- +200 documentos generados

**Estructura de documento:**
```javascript
{
    "id_cliente": 1000,
    "nombre": "Maria Soto",
    "region": "Metropolitana",
    "producto_comprado": "Trigo Granel Quintal",
    "calificacion": 4,
    "comentario": "Excelente calidad",
    "fecha": "2025-10-17"
}
```

**Uso:**
```powershell
mongosh < carga_mongodb.js
```

---

### üå°Ô∏è **datos_sensores_api.sql**
**Prop√≥sito:** Datos simulados de sensores IoT para API REST.

**Contenido:**
- Tabla: `sensores_data` en MySQL
- Registros de sensores ubicados en:
  - Invernaderos (A, B, Central)
  - Predios (Los Aromos, Santa Elena)
  - Campos abiertos
- Mediciones: temperatura, humedad, CO2, timestamp
- 250+ registros para noviembre 2025

**Uso:**
```powershell
mysql -u root -p AgroDataSur < datos_sensores_api.sql
```

---

### üîÑ **etl.py**
**Prop√≥sito:** Pipeline ETL (Extract, Transform, Load) para procesar datos desde m√∫ltiples fuentes hacia HDFS.

**Funcionalidades:**

#### 1. **Extracci√≥n desde MySQL**
- Consulta tabla `ventas` con JOINs
- Exporta a `ventas_data.csv`
- Sube a HDFS: `/user/hadoop/datos/ventas/`

#### 2. **Extracci√≥n desde MongoDB**
- Lee colecci√≥n `opiniones_clientes`
- Exporta a `opiniones_data.json` (formato JSON Lines)
- Sube a HDFS: `/user/hadoop/datos/opiniones/`

#### 3. **Extracci√≥n desde API REST**
- Consume endpoint: `http://localhost:3000/api/sensores_data`
- Exporta a `sensores_data.json`
- Sube a HDFS: `/user/hadoop/datos/sensores/`

**Configuraci√≥n requerida:**
```python
MYSQL_CONFIG = {
    'user': 'root',
    'password': 'tu_password_mysql',  # ‚ö†Ô∏è CAMBIAR
    'host': 'localhost',
    'database': 'AgroDataSur'
}

MONGO_URI = "mongodb://admin:tu_password_mongo@localhost:27017/"  # ‚ö†Ô∏è CAMBIAR
API_URL = "http://localhost:3000/api/sensores_data"
```

**Dependencias:**
```powershell
pip install mysql-connector-python pymongo requests
```

**Uso:**
```powershell
python etl.py
```

---

### üêù **consultas_hive.sql**
**Prop√≥sito:** Consultas anal√≠ticas sobre datos en Hive/Hadoop.

**Consultas implementadas:**

#### 1. **Temperatura Promedio por Ubicaci√≥n**
Analiza condiciones ambientales por invernadero/predio.

#### 2. **Satisfacci√≥n del Cliente por Producto**
Calcula calificaci√≥n promedio y total de opiniones por producto.

#### 3. **Tendencias de Ventas por Regi√≥n**
Identifica productos m√°s vendidos por regi√≥n geogr√°fica.

#### 4. **Rentabilidad Estimada por Temporada**
Calcula ingresos por estaci√≥n del a√±o (verano, oto√±o, invierno, primavera).

**Tablas Hive requeridas:**
- `sensores_hive` (datos de API)
- `opiniones_hive` (datos de MongoDB)
- `ventas_hive` (datos de MySQL)

**Uso:**
```bash
hive -f consultas_hive.sql
```

---

## üöÄ Flujo de Trabajo Completo

### **Paso 1: Preparaci√≥n**
```powershell
# Generar datos sint√©ticos
python generator.py
```

### **Paso 2: Configurar Servicios**
Ver secci√≥n [Configuraci√≥n de Servicios](#-configuraci√≥n-de-servicios).

### **Paso 3: Carga en MySQL**
```powershell
mysql -u root -p < mydb.sql
mysql -u root -p AgroDataSur < poblado_tablas_completo.sql
mysql -u root -p AgroDataSur < datos_sensores_api.sql
```

### **Paso 4: Carga en MongoDB**
```powershell
mongosh < carga_mongodb.js
```

### **Paso 5: Configurar API REST**
Implementar servidor que exponga `/api/sensores_data` desde tabla MySQL.

### **Paso 6: Ejecutar ETL**
```powershell
# Configurar credenciales en etl.py
python etl.py
```

### **Paso 7: Crear Tablas Hive**
```bash
# En Hive CLI
CREATE EXTERNAL TABLE ventas_hive (...)
LOCATION '/user/hadoop/datos/ventas/';

CREATE EXTERNAL TABLE opiniones_hive (...)
LOCATION '/user/hadoop/datos/opiniones/';

CREATE EXTERNAL TABLE sensores_hive (...)
LOCATION '/user/hadoop/datos/sensores/';
```

### **Paso 8: An√°lisis**
```bash
hive -f consultas_hive.sql
```

---

## üì¶ Dependencias

### Python
- `mysql-connector-python`: Conexi√≥n a MySQL
- `pymongo`: Conexi√≥n a MongoDB
- `requests`: Consumo de API REST

### Bases de Datos
- MySQL 8.0+
- MongoDB 6.0+

### Big Data
- Hadoop 3.x
- Hive 3.x

---

## ‚öôÔ∏è Configuraci√≥n de Servicios

El directorio `ArchivoConfiguracion/` contiene todas las configuraciones necesarias para el entorno Big Data.

### üêò **HDFS - Hadoop Distributed File System**

#### **core-site.xml**
Configuraci√≥n del sistema de archivos principal.

**Propiedades clave:**
```xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://localhost:9000</value>
</property>
```
- Define el NameNode en puerto 9000
- Configuraci√≥n de proxy user para ec2-user

**Ubicaci√≥n:** `$HADOOP_HOME/etc/hadoop/core-site.xml`

---

#### **hdfs-site.xml**
Configuraci√≥n espec√≠fica de HDFS.

**Propiedades clave:**
```xml
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
```
- Factor de replicaci√≥n: 1 (modo single-node)

**Ubicaci√≥n:** `$HADOOP_HOME/etc/hadoop/hdfs-site.xml`

---

#### **mapred-site.xml**
Configuraci√≥n de MapReduce.

**Propiedades clave:**
```xml
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
```
- MapReduce ejecuta sobre YARN

**Ubicaci√≥n:** `$HADOOP_HOME/etc/hadoop/mapred-site.xml`

---

#### **yarn-site.xml**
Configuraci√≥n de YARN (gesti√≥n de recursos).

**Propiedades clave:**
```xml
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
```
- Habilita shuffle service para MapReduce

**Ubicaci√≥n:** `$HADOOP_HOME/etc/hadoop/yarn-site.xml`

---

### üêù **Hive - Data Warehouse**

#### **hive-site.xml**
Configuraci√≥n de Hive para consultas anal√≠ticas.

**Propiedades principales:**

| Propiedad | Valor | Descripci√≥n |
|-----------|-------|-------------|
| `hive.metastore.warehouse.dir` | `hdfs://localhost:9000/user/hive/warehouse` | Ubicaci√≥n del warehouse |
| `javax.jdo.option.ConnectionURL` | `jdbc:derby:;databaseName=metastore_db;create=true` | Metastore embebido (Derby) |
| `hive.exec.scratchdir` | `/tmp/hive` | Directorio temporal |

**Ubicaci√≥n:** `$HIVE_HOME/conf/hive-site.xml`

**Nota:** Usa Derby embebido para desarrollo. Para producci√≥n considerar MySQL/PostgreSQL.

---

### üçÉ **MongoDB - Base de Datos NoSQL**

#### **config.yml**
Configuraci√≥n principal de MongoDB.

**Contenido:**
```yaml
storage:
  engine: wiredTiger              # Motor de almacenamiento
  dbPath: /var/lib/mongod/data    # Directorio de datos

systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true

net:
  bindIp: 0.0.0.0                 # Acepta conexiones remotas
  port: 27017

security:
  authorization: enabled           # Autenticaci√≥n habilitada

setParameter:
  enableLocalhostAuthBypass: true  # Permite crear admin inicial
```

**Ubicaci√≥n:** `/var/lib/mongod/config.yml`

---

#### **mongod.service**
Servicio systemd para MongoDB.

**Contenido:**
```ini
[Unit]
Description=MongoDB Server
After=network.target

[Service]
Type=simple
User=mongo
ExecStart=/usr/local/bin/mongod --config /var/lib/mongod/config.yml
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

**Instalaci√≥n del servicio:**
```bash
sudo cp ArchivoConfiguracion/MONGODB/mongod.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable mongod
sudo systemctl start mongod
```

**Verificar estado:**
```bash
sudo systemctl status mongod
```

---

## üîß Configuraci√≥n de Aplicaci√≥n

### Credenciales a Modificar

**En `etl.py`:**
- `MYSQL_CONFIG['password']`
- `MONGO_URI` (credenciales de admin)
- `API_URL` (si cambia el puerto/host)

### Rutas HDFS
Por defecto: `/user/hadoop/datos/`

Aseg√∫rate de crear los directorios:
```bash
hdfs dfs -mkdir -p /user/hadoop/datos/ventas
hdfs dfs -mkdir -p /user/hadoop/datos/opiniones
hdfs dfs -mkdir -p /user/hadoop/datos/sensores
```

### Inicializaci√≥n de Servicios

#### Hadoop/HDFS
```bash
# Formatear NameNode (solo primera vez)
hdfs namenode -format

# Iniciar servicios
start-dfs.sh
start-yarn.sh

# Verificar
jps  # Debe mostrar: NameNode, DataNode, ResourceManager, NodeManager
```

#### Hive
```bash
# Inicializar schema de metastore (solo primera vez)
schematool -dbType derby -initSchema

# Crear directorios en HDFS
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir -p /tmp/hive
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -chmod g+w /tmp/hive

# Iniciar Hive
hive
```

#### MongoDB
```bash
# Crear usuario administrador (solo primera vez)
mongosh
use admin
db.createUser({
  user: "admin",
  pwd: "tu_password",
  roles: ["root"]
})
exit

# Reiniciar servicio
sudo systemctl restart mongod
```

---

## üìä Datos Generados

| Componente | Cantidad | Formato |
|-----------|----------|---------|
| Productos | 10 | SQL/JSON |
| Clientes | 50 | SQL |
| Ventas | 220+ | SQL/CSV |
| Opiniones | 220+ | JSON |
| Sensores | 250+ | SQL/JSON |
| Sucursales | 6 | ‚Äî |
| Regiones | 4 | ‚Äî |

---

## üîç Verificaci√≥n del Sistema

### Comprobar servicios activos

```bash
# HDFS
hdfs dfsadmin -report

# YARN
yarn node -list

# MongoDB
mongosh --eval "db.adminCommand('ping')"

# MySQL
mysql -u root -p -e "SHOW DATABASES;"
```

### Verificar datos en HDFS

```bash
# Listar archivos
hdfs dfs -ls /user/hadoop/datos/ventas/
hdfs dfs -ls /user/hadoop/datos/opiniones/
hdfs dfs -ls /user/hadoop/datos/sensores/

# Ver contenido (primeras l√≠neas)
hdfs dfs -cat /user/hadoop/datos/ventas/ventas_data.csv | head
```

---

## üìù Notas T√©cnicas

### C√≥digos de Productos
Formato: `XXX-YY-NNN` (Tipo-Descripci√≥n-N√∫mero)

Ejemplos:
- `MAN-FJ-001`: Manzanas Fuji
- `ARD-PR-002`: Ar√°ndanos Premium
- `TRG-GR-004`: Trigo Granel

### Datos Temporales
- Las fechas de ventas abarcan enero-noviembre 2025
- Los datos de sensores son de noviembre 2025
- Los RUTs son sint√©ticos pero siguen formato chileno

### Caracter√≠sticas del Sistema
- **Modo de despliegue:** Single-node (desarrollo)
- **Replicaci√≥n HDFS:** Factor 1
- **Metastore Hive:** Derby embebido
- **MongoDB:** Autenticaci√≥n habilitada
- **Cumplimiento:** +200 registros por fuente de datos

---

## üö® Troubleshooting

### Problema: HDFS no inicia
```bash
# Ver logs
cat $HADOOP_HOME/logs/hadoop-*-namenode-*.log

# Reformatear (‚ö†Ô∏è BORRA DATOS)
stop-dfs.sh
rm -rf /tmp/hadoop-*
hdfs namenode -format
start-dfs.sh
```

### Problema: MongoDB no acepta conexiones
```bash
# Verificar puerto
sudo netstat -tlnp | grep 27017

# Ver logs
sudo tail -f /var/log/mongodb/mongod.log

# Revisar permisos
sudo chown -R mongo:mongo /var/lib/mongod
```

### Problema: Hive no encuentra tablas
```bash
# Verificar metastore
ls -la metastore_db/

# Reinicializar (‚ö†Ô∏è BORRA METADATOS)
rm -rf metastore_db/
schematool -dbType derby -initSchema
```

---

## üìö Referencias

### Documentaci√≥n Oficial
- [Hadoop Documentation](https://hadoop.apache.org/docs/)
- [Hive Documentation](https://hive.apache.org/)
- [MongoDB Manual](https://docs.mongodb.com/)

### Puertos por Defecto
| Servicio | Puerto | URL |
|----------|--------|-----|
| HDFS NameNode | 9000 | hdfs://localhost:9000 |
| HDFS NameNode Web UI | 9870 | http://localhost:9870 |
| YARN ResourceManager | 8088 | http://localhost:8088 |
| MongoDB | 27017 | mongodb://localhost:27017 |
| MySQL | 3306 | localhost:3306 |
| API Sensores | 3000 | http://localhost:3000 |

---

## üë• Autor

Proyecto desarrollado para evaluaci√≥n de Big Data - AgroDataSur

---

## üìÑ Licencia

Proyecto acad√©mico - Uso educativo
